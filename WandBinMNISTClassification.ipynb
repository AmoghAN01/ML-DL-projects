{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST ConvNet Classifier with WandB Integration \n",
    "# \n",
    "## Overview:\n",
    "This project implements a Convolutional Neural Network(CNN) to classify handwritten digits from the MNIST dataset using PyTorch.The project leverages PyTorch's powerful deep learning framework to build and train the CNN model efficiently. A key feature is the integration of Weights & Biases (wandb) for comprehensive experiment tracking and visualization, allowing for detailed monitoring of the training process and model performance. This implementation not only showcases the effectiveness of CNNs in recognizing handwritten digits but also highlights modern practices in machine learning development, including the use of advanced tools for performance tracking and analysis. \n",
    "\n",
    "## Requirements\n",
    "Python 3.x\n",
    "\n",
    "PyTorch\n",
    "\n",
    "torchvision\n",
    "\n",
    "matplotlib\n",
    "\n",
    "numpy\n",
    "\n",
    "wandb\n",
    "\n",
    "## Methodology:\n",
    "The project follows a structured approach to build and train the CNN model. It starts by loading and preprocessing the MNIST dataset using PyTorch's data loaders. The CNN architecture is then defined, consisting of two convolutional layers and two fully connected layers, with ReLU activation and max pooling. The model is trained using the Adam optimizer and Negative Log Likelihood Loss function. The training process is conducted over 10 epochs with a batch size of 512. Importantly, the project integrates Weights & Biases (wandb) for comprehensive experiment tracking and visualization, allowing for detailed monitoring of the training process and model performance.\n",
    "\n",
    "## Results:\n",
    "The MNIST ConvNet Classifier achieves impressive results after 10 epochs of training. The model attains a test accuracy of 98.95% and a test loss of 0.03433, demonstrating its effectiveness in classifying handwritten digits. These results are meticulously tracked and visualized using wandb, providing comprehensive insights into the model's performance over time. The integration of wandb not only facilitates easy monitoring of training progress but also enables detailed analysis of loss and accuracy metrics, making it an invaluable tool for model development and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamogh-ajith\u001b[0m (\u001b[33mamogh-ajith-amrita-vishwa-vidyapeetham\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\91963\\wandb\\run-20241129_033325-shmt47e6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet/runs/shmt47e6' target=\"_blank\">ConvNet_MNIST</a></strong> to <a href='https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet' target=\"_blank\">https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet/runs/shmt47e6' target=\"_blank\">https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet/runs/shmt47e6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"mnist_convnet\", name=\"ConvNet_MNIST\")\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_images(data_loader):\n",
    "    batch = next(iter(data_loader))  \n",
    "    images, labels = batch  \n",
    "    images = images[:5]\n",
    "    labels = labels[:5]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(12, 2))\n",
    "    for i in range(5):\n",
    "        image = images[i].numpy().squeeze()\n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {labels[i].item()}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3)\n",
    "        self.fc1 = nn.Linear(20*10*10, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = out.view(in_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    wandb.log({\"test_loss\": test_loss, \"accuracy\": accuracy})\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({accuracy:.0f}%)\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.354362\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.183917\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.163156\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.085585\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.074752\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.096309\n",
      "\n",
      "Test set: Average loss: 0.0567, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.036750\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.063170\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.061121\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.034941\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.043670\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.050178\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.024151\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.059742\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.030040\n",
      "\n",
      "Test set: Average loss: 0.0356, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.012252\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.014493\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.026844\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.008836\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.015295\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.016103\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.014661\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.035103\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.014297\n",
      "\n",
      "Test set: Average loss: 0.0366, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.008238\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.007442\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.026336\n",
      "\n",
      "Test set: Average loss: 0.0360, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.013770\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.006984\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.006121\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 9895/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▆▇██▇██</td></tr><tr><td>test_loss</td><td>█▄▃▃▁▁▁▂▁▁</td></tr><tr><td>train_loss</td><td>█▄▄▄▃▃▃▂▃▃▂▂▂▂▁▂▂▂▂▁▂▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>98.95</td></tr><tr><td>test_loss</td><td>0.03433</td></tr><tr><td>train_loss</td><td>0.00511</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ConvNet_MNIST</strong> at: <a href='https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet/runs/shmt47e6' target=\"_blank\">https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet/runs/shmt47e6</a><br/> View project at: <a href='https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet' target=\"_blank\">https://wandb.ai/amogh-ajith-amrita-vishwa-vidyapeetham/mnist_convnet</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241129_033325-shmt47e6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader)\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
